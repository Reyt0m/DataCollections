#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Portfolio Scraper Tool (Improved Version)
URLãƒªã‚¹ãƒˆã‹ã‚‰HTMLæ§‹æˆè¦ç´ ã‚’é›†ã‚ã€portfolioã‚¿ãƒ–ã‚’è¦‹ã¤ã‘ã€ãã®ãƒšãƒ¼ã‚¸ã®ä¸­ã«ã‚ã‚‹ä¼šç¤¾åã‚’ã™ã¹ã¦é›†ã‚ã‚‹ãƒ„ãƒ¼ãƒ«
ç”»åƒãƒ™ãƒ¼ã‚¹ã®ä¼šç¤¾åã«ã‚‚å¯¾å¿œã—ã€OCRæ©Ÿèƒ½ã‚‚å«ã‚€
"""

import requests
import time
import json
import csv
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import re
from typing import List, Dict, Optional, Set, Tuple
import logging
import os
from PIL import Image
import io
import base64
import cv2
import numpy as np

# OCRé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import easyocr
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("Warning: easyocr not available. Install with: pip install easyocr")

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("Warning: pytesseract not available. Install with: pip install pytesseract")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PortfolioScraper:
    def __init__(self, headless=True, timeout=10, use_ocr=False):
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–

        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
            use_ocr: OCRæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.headless = headless
        self.timeout = timeout
        self.use_ocr = use_ocr and (OCR_AVAILABLE or TESSERACT_AVAILABLE)
        self.driver = None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        # OCRæ©Ÿèƒ½ã®åˆæœŸåŒ–
        self.ocr_reader = None
        if self.use_ocr:
            self._initialize_ocr()

        # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        self._initialize_driver()

        # æ‹¡å¼µã•ã‚ŒãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.portfolio_keywords = [
            'portfolio', 'ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª', 'æŠ•è³‡å…ˆ', 'ä¼æ¥­', 'ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼',
            'investments', 'companies', 'partners', 'clients',
            'investment', 'invest', 'å‡ºè³‡å…ˆ', 'æŠ•è³‡ä¼æ¥­', 'æŠ•è³‡å®Ÿç¸¾',
            'portfolio companies', 'portfolio companies', 'æŠ•è³‡å¯¾è±¡ä¼æ¥­'
        ]

        # ä¼šç¤¾åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹å–„ç‰ˆï¼‰
        self.company_patterns = [
            # æ—¥æœ¬èªã®ä¼šç¤¾å½¢æ…‹
            r'æ ªå¼ä¼šç¤¾\s*([^\s]+)',
            r'æœ‰é™ä¼šç¤¾\s*([^\s]+)',
            r'åˆåŒä¼šç¤¾\s*([^\s]+)',
            r'([^\s]+)\s*æ ªå¼ä¼šç¤¾',
            r'([^\s]+)\s*æœ‰é™ä¼šç¤¾',
            r'([^\s]+)\s*åˆåŒä¼šç¤¾',
            r'([^\s]+)\s*ãˆ±',
            r'([^\s]+)\s*ãˆ²',
            r'([^\s]+)\s*ãˆ³',

            # è‹±èªã®ä¼šç¤¾å½¢æ…‹
            r'([^\s]+)\s*Inc\.',
            r'([^\s]+)\s*Corp\.',
            r'([^\s]+)\s*LLC',
            r'([^\s]+)\s*Ltd\.',
            r'([^\s]+)\s*Co\.',
            r'([^\s]+)\s*Company',
            r'([^\s]+)\s*Technologies',
            r'([^\s]+)\s*Systems',
            r'([^\s]+)\s*Solutions',
            r'([^\s]+)\s*Group',
            r'([^\s]+)\s*Partners',
            r'([^\s]+)\s*Ventures',
            r'([^\s]+)\s*Capital',
            r'([^\s]+)\s*Fund',
            r'([^\s]+)\s*Studio',
            r'([^\s]+)\s*Labs',
            r'([^\s]+)\s*Works',
            r'([^\s]+)\s*Services',
            r'([^\s]+)\s*Platform',
            r'([^\s]+)\s*Network',
            r'([^\s]+)\s*Media',
            r'([^\s]+)\s*Digital',
            r'([^\s]+)\s*Tech',
            r'([^\s]+)\s*AI',
            r'([^\s]+)\s*Bio',
            r'([^\s]+)\s*Health',
            r'([^\s]+)\s*Care',
            r'([^\s]+)\s*Life',
            r'([^\s]+)\s*Food',
            r'([^\s]+)\s*Energy',
            r'([^\s]+)\s*Green',
            r'([^\s]+)\s*Eco',
            r'([^\s]+)\s*Smart',
            r'([^\s]+)\s*Next',
            r'([^\s]+)\s*Future',
            r'([^\s]+)\s*Global',
            r'([^\s]+)\s*World',
            r'([^\s]+)\s*International',
            r'([^\s]+)\s*Japan',
            r'([^\s]+)\s*Asia',
            r'([^\s]+)\s*Pacific',
            r'([^\s]+)\s*America',
            r'([^\s]+)\s*Europe',

            # ä¸€èˆ¬çš„ãªä¼šç¤¾åãƒ‘ã‚¿ãƒ¼ãƒ³
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',  # ã‚­ãƒ£ãƒ”ã‚¿ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå˜èª
            r'([A-Z]{2,}(?:\s+[A-Z]{2,})*)',  # å¤§æ–‡å­—ã®ç•¥èª
        ]

    def _initialize_ocr(self):
        """OCRæ©Ÿèƒ½ã®åˆæœŸåŒ–"""
        try:
            if OCR_AVAILABLE:
                import easyocr
                self.ocr_reader = easyocr.Reader(['ja', 'en'])
                logger.info("EasyOCR initialized successfully")
            elif TESSERACT_AVAILABLE:
                logger.info("Tesseract available for OCR")
            else:
                logger.warning("No OCR engine available")
        except Exception as e:
            logger.error(f"OCRåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.ocr_reader = None

    def _initialize_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            # Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            # è¿½åŠ ã®å®‰å®šæ€§ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-images')
            chrome_options.add_argument('--disable-javascript')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--allow-running-insecure-content')

            # WebDriver Managerã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—
            try:
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.driver.set_page_load_timeout(self.timeout)
                self.driver.implicitly_wait(10)
                logger.info("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"WebDriver Managerã§ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ã®ChromeDriverã‚’ä½¿ç”¨
                try:
                    self.driver = webdriver.Chrome(options=chrome_options)
                    self.driver.set_page_load_timeout(self.timeout)
                    self.driver.implicitly_wait(10)
                    logger.info("ã‚·ã‚¹ãƒ†ãƒ ã®ChromeDriverã§åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
                except Exception as e2:
                    logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã®ChromeDriverã§ã‚‚åˆæœŸåŒ–ã«å¤±æ•—: {e2}")
                    self.driver = None

        except Exception as e:
            logger.error(f"Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            self.driver = None

    def setup_ocr(self):
        """OCRæ©Ÿèƒ½ã®åˆæœŸåŒ–"""
        self.ocr_reader = None
        if OCR_AVAILABLE:
            try:
                # æ—¥æœ¬èªã¨è‹±èªã«å¯¾å¿œ
                self.ocr_reader = easyocr.Reader(['ja', 'en'], gpu=False)
                logger.info("EasyOCR initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize EasyOCR: {e}")
                self.ocr_reader = None

    def setup_driver(self, headless: bool):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®è¨­å®š"""
        try:
            chrome_options = Options()
            if headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            logger.info("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            self.driver = None

    def find_portfolio_tab(self, soup, base_url: str) -> Optional[str]:
        """
        ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¿ãƒ–ã‚’æ¢ã™ï¼ˆæ”¹å–„ç‰ˆï¼‰

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            base_url: ãƒ™ãƒ¼ã‚¹URL

        Returns:
            ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªURLã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        # 1. ãƒªãƒ³ã‚¯è¦ç´ ã‹ã‚‰ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¿ãƒ–ã‚’æ¢ã™
        for link in soup.find_all('a', href=True):
            href = link.get('href', '').lower()
            text = link.get_text().lower()

            # æ‹¡å¼µã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            for keyword in self.portfolio_keywords:
                if keyword in href or keyword in text:
                    portfolio_url = urljoin(base_url, link['href'])
                    logger.info(f"Portfolioã‚¿ãƒ–ã‚’ç™ºè¦‹: {portfolio_url}")
                    return portfolio_url

            # ç‰¹æ®Šãªã‚±ãƒ¼ã‚¹: ANRIã®ã‚ˆã†ãªä¼æ¥­
            if 'anri' in base_url.lower() and ('portfolio' in href or 'companies' in href):
                portfolio_url = urljoin(base_url, link['href'])
                logger.info(f"ANRIç‰¹æ®Šã‚±ãƒ¼ã‚¹ - Portfolioã‚¿ãƒ–ã‚’ç™ºè¦‹: {portfolio_url}")
                return portfolio_url

        # 2. ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ãŒãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒšãƒ¼ã‚¸ã‹ãƒã‚§ãƒƒã‚¯
        current_url = base_url.lower()
        for keyword in self.portfolio_keywords:
            if keyword in current_url:
                logger.info(f"ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ãŒãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒšãƒ¼ã‚¸: {base_url}")
                return base_url

        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæƒ…å ±ã‚’æ¢ã™
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            content = meta.get('content', '').lower()
            if any(keyword in content for keyword in self.portfolio_keywords):
                logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæƒ…å ±ã‚’ç™ºè¦‹: {content}")
                return base_url

        logger.warning(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¿ãƒ–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_url}")
        return None

    def extract_text_from_image(self, img_url: str) -> Optional[str]:
        """
        ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆOCRï¼‰- æ”¹å–„ç‰ˆ

        Args:
            img_url: ç”»åƒã®URL

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        if not self.use_ocr:
            return None

        try:
            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = self.session.get(img_url, timeout=15)
            response.raise_for_status()

            # PILã§ç”»åƒã‚’é–‹ã
            img = Image.open(io.BytesIO(response.content))

            # ç”»åƒã®å“è³ªãƒã‚§ãƒƒã‚¯
            if not self._is_image_quality_good(img):
                logger.debug(f"ç”»åƒå“è³ªãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {img_url}")
                return None

            # ç”»åƒã®å‰å‡¦ç†
            processed_img = self._preprocess_image(img)

            # è¤‡æ•°ã®OCRã‚¨ãƒ³ã‚¸ãƒ³ã§è©¦è¡Œ
            extracted_text = self._try_multiple_ocr(processed_img, img_url)

            if extracted_text:
                # ãƒ†ã‚­ã‚¹ãƒˆã®å¾Œå‡¦ç†
                cleaned_text = self._postprocess_text(extracted_text)
                if cleaned_text:
                    logger.debug(f"OCRæˆåŠŸ: {img_url} -> {cleaned_text}")
                    return cleaned_text

        except Exception as e:
            logger.warning(f"ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—: {img_url} - {e}")

        return None

    def _is_image_quality_good(self, img: Image.Image) -> bool:
        """
        ç”»åƒã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            img: PILç”»åƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            å“è³ªãŒè‰¯ã„å ´åˆã¯True
        """
        try:
            # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            width, height = img.size
            if width < 50 or height < 50:
                return False

            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ã«ç´°é•·ã„ç”»åƒã¯é™¤å¤–ï¼‰
            aspect_ratio = width / height
            if aspect_ratio > 10 or aspect_ratio < 0.1:
                return False

            # ç”»åƒãƒ¢ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if img.mode not in ['RGB', 'RGBA', 'L', 'P']:
                return False

            return True

        except Exception:
            return False

    def _preprocess_image(self, img: Image.Image) -> Image.Image:
        """
        ç”»åƒã®å‰å‡¦ç†

        Args:
            img: å…ƒã®ç”»åƒ

        Returns:
            å‰å‡¦ç†ã•ã‚ŒãŸç”»åƒ
        """
        try:
            # RGBã«å¤‰æ›
            if img.mode != 'RGB':
                img = img.convert('RGB')

            # ç”»åƒã‚µã‚¤ã‚ºã®èª¿æ•´ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯ç¸®å°ï¼‰
            max_size = 1024
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = tuple(int(dim * ratio) for dim in img.size)
                img = img.resize(new_size, Image.Resampling.LANCZOS)

            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã¨æ˜åº¦ã®èª¿æ•´
            from PIL import ImageEnhance

            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’å°‘ã—ä¸Šã’ã‚‹
            enhancer = ImageEnhance.Contrast(img)
            img = enhancer.enhance(1.2)

            # æ˜åº¦ã‚’èª¿æ•´
            enhancer = ImageEnhance.Brightness(img)
            img = enhancer.enhance(1.1)

            return img

        except Exception as e:
            logger.debug(f"ç”»åƒå‰å‡¦ç†ã«å¤±æ•—: {e}")
            return img

    def _try_multiple_ocr(self, img: Image.Image, img_url: str) -> Optional[str]:
        """
        è¤‡æ•°ã®OCRã‚¨ãƒ³ã‚¸ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦è¡Œ

        Args:
            img: å‰å‡¦ç†ã•ã‚ŒãŸç”»åƒ
            img_url: ç”»åƒURLï¼ˆãƒ­ã‚°ç”¨ï¼‰

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # 1. EasyOCRã‚’è©¦è¡Œ
        if self.ocr_reader:
            try:
                results = self.ocr_reader.readtext(np.array(img))
                texts = []
                for result in results:
                    text, confidence = result[1], result[2]
                    # ä¿¡é ¼åº¦ãŒ50%ä»¥ä¸Šã®å ´åˆã®ã¿ä½¿ç”¨
                    if confidence > 0.5 and len(text.strip()) > 1:
                        texts.append(text.strip())

                if texts:
                    combined_text = ' '.join(texts)
                    logger.debug(f"EasyOCRæˆåŠŸ: {img_url} -> {combined_text}")
                    return combined_text

            except Exception as e:
                logger.debug(f"EasyOCRå¤±æ•—: {img_url} - {e}")

        # 2. Tesseractã‚’è©¦è¡Œ
        if TESSERACT_AVAILABLE:
            try:
                # æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã§è©¦è¡Œ
                text_jp = pytesseract.image_to_string(img, lang='jpn+eng')
                text_en = pytesseract.image_to_string(img, lang='eng')

                # ã‚ˆã‚Šé•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
                if len(text_jp.strip()) > len(text_en.strip()):
                    text = text_jp
                else:
                    text = text_en

                if text.strip():
                    logger.debug(f"TesseractæˆåŠŸ: {img_url} -> {text.strip()}")
                    return text.strip()

            except Exception as e:
                logger.debug(f"Tesseractå¤±æ•—: {img_url} - {e}")

        # 3. ç”»åƒã®altå±æ€§ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        return self._extract_text_from_filename(img_url)

    def _extract_text_from_filename(self, img_url: str) -> Optional[str]:
        """
        ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚„URLã‹ã‚‰ä¼šç¤¾åã‚’æ¨æ¸¬

        Args:
            img_url: ç”»åƒURL

        Returns:
            æ¨æ¸¬ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
            filename = img_url.split('/')[-1].split('?')[0]

            # æ‹¡å¼µå­ã‚’é™¤å»
            name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename

            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚„ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
            name_cleaned = name_without_ext.replace('_', ' ').replace('-', ' ').replace('+', ' ')

            # æ•°å­—ã‚„ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
            import re
            name_cleaned = re.sub(r'[0-9]+', '', name_cleaned)
            name_cleaned = re.sub(r'[^\w\s]', '', name_cleaned)

            # è¤‡æ•°ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
            name_cleaned = ' '.join(name_cleaned.split())

            if len(name_cleaned) > 2:
                logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬: {img_url} -> {name_cleaned}")
                return name_cleaned

        except Exception as e:
            logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«åè§£æå¤±æ•—: {img_url} - {e}")

        return None

    def _postprocess_text(self, text: str) -> Optional[str]:
        """
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å¾Œå‡¦ç†

        Args:
            text: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            å¾Œå‡¦ç†ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            text = text.strip()

            # æ”¹è¡Œã‚„ã‚¿ãƒ–ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
            text = re.sub(r'[\n\r\t]+', ' ', text)

            # è¤‡æ•°ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
            text = re.sub(r'\s+', ' ', text)

            # ç‰¹æ®Šæ–‡å­—ã®é™¤å»ï¼ˆãŸã ã—æ—¥æœ¬èªã¨è‹±èªã¯ä¿æŒï¼‰
            text = re.sub(r'[^\w\s\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', '', text)

            # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
            if len(text) < 2:
                return None

            # æ˜ã‚‰ã‹ã«ä¼šç¤¾åã§ãªã„ã‚‚ã®ã‚’é™¤å¤–
            exclude_patterns = [
                r'^[0-9]+$',  # æ•°å­—ã®ã¿
                r'^[a-zA-Z]{1,2}$',  # 1-2æ–‡å­—ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ
                r'^(click|read|more|view|learn|see|home|about|contact)$',  # ä¸€èˆ¬çš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç”¨èª
                r'^(logo|image|photo|picture|icon)$',  # ç”»åƒé–¢é€£ç”¨èª
            ]

            for pattern in exclude_patterns:
                if re.match(pattern, text.lower()):
                    return None

            return text

        except Exception as e:
            logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆå¾Œå‡¦ç†å¤±æ•—: {e}")
            return None

    def click_image_and_extract_company(self, img_element, base_url: str) -> Optional[str]:
        """
        ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º

        Args:
            img_element: ç”»åƒè¦ç´ 
            base_url: ãƒ™ãƒ¼ã‚¹URL

        Returns:
            ä¼šç¤¾åã€å¤±æ•—æ™‚ã¯None
        """
        if not self.driver:
            return None

        try:
            # ç”»åƒã®è¦ªè¦ç´ ãŒãƒªãƒ³ã‚¯ã‹ãƒã‚§ãƒƒã‚¯
            parent_link = img_element.find_parent('a')
            if not parent_link:
                return None

            href = parent_link.get('href')
            if not href:
                return None

            detail_url = urljoin(base_url, href)

            # è©³ç´°ãƒšãƒ¼ã‚¸ã‚’é–‹ã
            self.driver.get(detail_url)
            time.sleep(2)

            # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
            detail_soup = BeautifulSoup(self.driver.page_source, 'lxml')
            companies = self.extract_companies_from_page(detail_soup)

            if companies:
                return list(companies)[0]  # æœ€åˆã®ä¼šç¤¾åã‚’è¿”ã™

        except Exception as e:
            logger.warning(f"ç”»åƒã‚¯ãƒªãƒƒã‚¯ã«ã‚ˆã‚‹è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—: {e}")

        return None

    def extract_companies_from_page(self, soup: BeautifulSoup, base_url: str = "") -> Set[str]:
        """
        ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡ºã™ã‚‹ï¼ˆå¤§å¹…æ”¹å–„ç‰ˆï¼‰

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            base_url: ãƒ™ãƒ¼ã‚¹URLï¼ˆç”»åƒã‚¯ãƒªãƒƒã‚¯ç”¨ï¼‰

        Returns:
            ä¼šç¤¾åã®ã‚»ãƒƒãƒˆ
        """
        companies = set()

        # 1. ç‰¹å®šã®ã‚¯ãƒ©ã‚¹åã‚’æŒã¤è¦ç´ ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡ºï¼ˆå„ªå…ˆåº¦æœ€é«˜ï¼‰
        portfolio_selectors = [
            '.fg-item-title',  # 15th Rock
            '.card_companyName__BWs6G',  # ANRI
            '.portfolioItem__title',  # ã‚µãƒ ãƒ©ã‚¤ã‚¤ãƒ³ã‚­ãƒ¥ãƒ™ãƒ¼ãƒˆ
            '.portfolio__item',  # ã‚¸ã‚§ãƒã‚·ã‚¢
            '[class*="company-name"]',
            '[class*="companyName"]',
            '[class*="fg-item-title"]',
            '[class*="card_companyName"]',
            '[class*="portfolio-item"]',
            '[class*="portfolioItem"]',
            'h2.fg-item-title',
            'h3.card_companyName__BWs6G',
            '.portfolio-item h2',
            '.portfolio-item h3',
            '.company-card h2',
            '.company-card h3',
            '.card h2',
            '.card h3',
            '.gallery-item h2',
            '.gallery-item h3',
            '.portfolio__item h3',
            '.portfolio__item h2'
        ]

        for selector in portfolio_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if text and len(text) > 1 and len(text) < 100:
                    clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', text).strip()
                    if clean_text:
                        companies.add(clean_text)

        # 2. ç”»åƒã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡ºï¼ˆOCRä½¿ç”¨ï¼‰
        if self.use_ocr:
            img_elements = soup.find_all('img')
            processed_images = 0
            successful_ocr = 0

            for img in img_elements:
                src = img.get('src', '')
                if src:
                    # ç”»åƒURLã‚’å®Œå…¨ãªURLã«å¤‰æ›
                    if src.startswith('//'):
                        img_url = 'https:' + src
                    elif src.startswith('/'):
                        img_url = urljoin(base_url, src)
                    elif not src.startswith('http'):
                        img_url = urljoin(base_url, src)
                    else:
                        img_url = src

                    # ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    extracted_text = self.extract_text_from_image(img_url)
                    processed_images += 1

                    if extracted_text:
                        successful_ocr += 1
                        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¼šç¤¾åã‚‰ã—ã„ã‚‚ã®ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        words = extracted_text.split()
                        for word in words:
                            if (len(word) >= 2 and len(word) <= 50 and
                                any(keyword in word.lower() for keyword in ['inc', 'corp', 'ltd', 'co', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾'])):
                                companies.add(word)

                        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚‚è¿½åŠ ï¼ˆä¼šç¤¾åã®å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆï¼‰
                        if len(extracted_text) >= 3 and len(extracted_text) <= 50:
                            companies.add(extracted_text)

                    # ç”»åƒã‚¯ãƒªãƒƒã‚¯ã«ã‚ˆã‚‹è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆé™å®šçš„ã«å®Ÿè¡Œï¼‰
                    if base_url and processed_images <= 10:  # æœ€åˆã®10æšã®ã¿
                        detail_company = self.click_image_and_extract_company(img, base_url)
                        if detail_company:
                            companies.add(detail_company)

            logger.info(f"ç”»åƒå‡¦ç†çµæœ: {processed_images}æšå‡¦ç†, {successful_ocr}æšã§OCRæˆåŠŸ")

        # 3. ãƒªãƒ³ã‚¯è¦ç´ ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
        link_selectors = [
            'a[href*="http"]',  # å¤–éƒ¨ãƒªãƒ³ã‚¯
            '.card a',
            '.portfolio-item a',
            '.company-card a',
            '.gallery-item a',
            '.portfolio__item a'
        ]

        for selector in link_selectors:
            elements = soup.select(selector)
            for element in elements:
                # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
                text = element.get_text(strip=True)
                if text and len(text) > 1 and len(text) < 100:
                    clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', text).strip()
                    if clean_text:
                        companies.add(clean_text)

                # altå±æ€§ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
                alt_text = element.get('alt', '')
                if alt_text and len(alt_text) > 1 and len(alt_text) < 100:
                    if any(keyword in alt_text.lower() for keyword in ['logo', 'company', 'corp', 'inc', 'ltd', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾']):
                        clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', alt_text).strip()
                        if clean_text:
                            companies.add(clean_text)

        # 4. ç”»åƒã®altå±æ€§ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
        img_elements = soup.find_all('img')
        for img in img_elements:
            alt_text = img.get('alt', '')
            if alt_text and len(alt_text) > 1 and len(alt_text) < 100:
                if any(keyword in alt_text.lower() for keyword in ['logo', 'company', 'corp', 'inc', 'ltd', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾']):
                    clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', alt_text).strip()
                    if clean_text:
                        companies.add(clean_text)

        # 5. è¦‹å‡ºã—è¦ç´ ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
        heading_selectors = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']
        for selector in heading_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if text and len(text) > 1 and len(text) < 100:
                    if any(keyword in text.lower() for keyword in ['inc', 'corp', 'ltd', 'co', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾']):
                        clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', text).strip()
                        if clean_text:
                            companies.add(clean_text)

        # 6. ãƒªã‚¹ãƒˆè¦ç´ ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
        list_elements = soup.find_all(['ul', 'ol'])
        for list_elem in list_elements:
            items = list_elem.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if text and len(text) > 1 and len(text) < 100:
                    if any(keyword in text.lower() for keyword in ['inc', 'corp', 'ltd', 'co', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'åˆåŒä¼šç¤¾']):
                        clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', text).strip()
                        if clean_text:
                            companies.add(clean_text)

        # 7. æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        text_content = soup.get_text()
        for pattern in self.company_patterns:
            matches = re.findall(pattern, text_content)
            for match in matches:
                if match and len(match.strip()) > 1 and len(match.strip()) < 100:
                    clean_text = re.sub(r'[ğŸ‡¯ğŸ‡µğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±ğŸ‡¨ğŸ‡¦ğŸ‡¬ğŸ‡§ğŸ‡ºğŸ‡¸ğŸ‡³ğŸ‡±]', '', match.strip()).strip()
                    if clean_text:
                        companies.add(clean_text)

        # æœ€çµ‚çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        companies = self._filter_company_names(companies)

        return companies

    def _filter_company_names(self, companies: Set[str]) -> Set[str]:
        """
        ä¼šç¤¾åã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ãƒã‚¤ã‚ºã‚’é™¤å»ï¼ˆæ”¹å–„ç‰ˆï¼‰

        Args:
            companies: æŠ½å‡ºã•ã‚ŒãŸä¼šç¤¾åã®ã‚»ãƒƒãƒˆ

        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸä¼šç¤¾åã®ã‚»ãƒƒãƒˆ
        """
        filtered_companies = set()

        # é™¤å¤–ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            # ä¸€èˆ¬çš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ 
            r'^(top|home|about|contact|news|blog|careers|privacy|terms|login|signup|search)$',
            r'^(menu|navigation|header|footer|sidebar|main|content)$',
            r'^(next|previous|back|forward|close|open|expand|collapse)$',

            # ä¸€èˆ¬çš„ãªå˜èª
            r'^(our|we|you|they|them|this|that|these|those)$',
            r'^(the|and|or|but|for|with|from|to|in|on|at|by)$',
            r'^(all|any|some|many|few|much|little|more|less)$',

            # æŠ€è¡“çš„ãªãƒã‚¤ã‚º
            r'^[a-f0-9]{8,}$',  # 16é€²æ•°æ–‡å­—åˆ—
            r'^[a-z]{1,2}$',  # 1-2æ–‡å­—ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ
            r'^[0-9]+$',  # æ•°å­—ã®ã¿
            r'^[a-z]+[0-9]+[a-z]+$',  # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ+æ•°å­—+ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ

            # OCRãƒã‚¤ã‚º
            r'^[a-z]{3,}[a-z]{3,}[a-z]{3,}$',  # ç¹°ã‚Šè¿”ã—æ–‡å­—
            r'^[a-z]+[a-z]+[a-z]+[a-z]+$',  # 4å›ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—

            # ç”»åƒãƒ»ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£
            r'\.(png|jpg|jpeg|gif|svg|ico|webp)$',
            r'^(logo|image|photo|picture|icon|img)$',
            r'^(download|upload|file|document|pdf)$',
            r'.*logo.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # ãƒ­ã‚´ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*_logo.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # _logoã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*logo_.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # logo_ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*_edited.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # _editedã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*_2025.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # _2025ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*_2024.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # _2024ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«
            r'.*_2023.*\.(png|jpg|jpeg|gif|svg|ico|webp)$',  # _2023ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«

            # è¨€èªãƒ»åœ°åŸŸé–¢é€£
            r'^(en|jp|ja|us|uk|eu|asia|pacific|global|world)$',
            r'^(english|japanese|chinese|korean|spanish|french|german)$',

            # æ—¥ä»˜ãƒ»æ™‚åˆ»
            r'^\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'^\d{2}:\d{2}:\d{2}',  # HH:MM:SS
            r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',  # ISOå½¢å¼

            # ç‰¹æ®Šæ–‡å­—ã®ã¿
            r'^[^\w\s]+$',

            # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            r'^.{1,2}$',

            # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆï¼ˆèª¬æ˜æ–‡ãªã©ï¼‰
            r'^.{100,}$',
        ]

        # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå³ã—ã•ã‚’èª¿æ•´ï¼‰
        exclude_keywords = {
            'copyright', 'privacy', 'terms', 'policy', 'legal', 'disclaimer',
            'top', 'home', 'about', 'contact', 'news', 'blog', 'careers',
            'menu', 'navigation', 'header', 'footer', 'sidebar',
            'next', 'previous', 'back', 'forward', 'close', 'open',
            'our', 'we', 'you', 'they', 'them', 'this', 'that',
            'logo', 'image', 'photo', 'picture', 'icon', 'img',
            'download', 'upload', 'file', 'document',
            'en', 'jp', 'ja', 'us', 'uk', 'eu', 'asia', 'pacific',
            'english', 'japanese', 'chinese', 'korean',
            'all', 'any', 'some', 'many', 'few', 'much', 'little',
            'the', 'and', 'or', 'but', 'for', 'with', 'from', 'to', 'in', 'on', 'at', 'by',
            # UIè¦ç´ ï¼ˆå³ã—ã•ã‚’èª¿æ•´ï¼‰
            'website', 'location', 'chevron', 'right', 'left', 'up', 'down',
            'general', 'partner', 'lead', 'position', 'principal', 'associate',
            'founder', 'ceo', 'cto', 'cfo', 'coo', 'director', 'manager',
            'team', 'member', 'staff', 'employee', 'consultant', 'advisor',
            'board', 'committee', 'council', 'group', 'division', 'department',
            'section', 'unit', 'branch', 'office', 'location', 'address',
            'phone', 'email', 'contact', 'support', 'help', 'info', 'information',
            'service', 'services', 'product', 'products', 'solution', 'solutions',
            'technology', 'technologies', 'innovation', 'research', 'development',
            'investment', 'investments', 'portfolio', 'portfolios', 'company', 'companies',
            'corporation', 'corporations', 'limited', 'incorporated', 'partnership',
            'venture', 'ventures', 'capital', 'fund', 'funds', 'asset', 'assets',
            'management', 'consulting', 'advisory', 'financial', 'banking',
            'insurance', 'real estate', 'property', 'development', 'construction',
            'manufacturing', 'production', 'distribution', 'retail', 'wholesale',
            'trade', 'commerce', 'business', 'enterprise', 'startup', 'startups',
            'scaleup', 'scaleups', 'growth', 'expansion', 'acquisition', 'merger',
            'exit', 'exits', 'ipo', 'm&a', 'ma', 'deal', 'deals', 'transaction',
            'round', 'series', 'seed', 'angel', 'pre-seed', 'pre-seed', 'pre-seed',
            # å˜ä¸€æ–‡å­—ã¯é™¤å¤–
            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'
        }

        for company in companies:
            company_lower = company.lower().strip()

            # åŸºæœ¬çš„ãªé•·ã•ãƒã‚§ãƒƒã‚¯
            if len(company_lower) < 3 or len(company_lower) > 50:
                continue

            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if company_lower in exclude_keywords:
                continue

            # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            should_exclude = False
            for pattern in exclude_patterns:
                if re.match(pattern, company_lower):
                    should_exclude = True
                    break

            if should_exclude:
                continue

            # ä¼šç¤¾åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            company_patterns = [
                r'.*æ ªå¼ä¼šç¤¾.*',
                r'.*æœ‰é™ä¼šç¤¾.*',
                r'.*åˆåŒä¼šç¤¾.*',
                r'.*Inc\.?$',
                r'.*Corp\.?$',
                r'.*LLC$',
                r'.*Ltd\.?$',
                r'.*Co\.?$',
                r'.*Company$',
                r'.*Technologies$',
                r'.*Systems$',
                r'.*Solutions$',
                r'.*Group$',
                r'.*Partners$',
                r'.*Ventures$',
                r'.*Capital$',
                r'.*Fund$',
                r'.*Studio$',
                r'.*Labs$',
                r'.*Works$',
                r'.*Services$',
                r'.*Platform$',
                r'.*Network$',
                r'.*Media$',
                r'.*Digital$',
                r'.*Tech$',
                r'.*AI$',
                r'.*Bio$',
                r'.*Health$',
                r'.*Care$',
                r'.*Life$',
                r'.*Food$',
                r'.*Energy$',
                r'.*Green$',
                r'.*Eco$',
                r'.*Smart$',
                r'.*Next$',
                r'.*Future$',
                r'.*Global$',
                r'.*World$',
                r'.*International$',
                r'.*Japan$',
                r'.*Asia$',
                r'.*Pacific$',
                r'.*America$',
                r'.*Europe$',
            ]

            # ä¼šç¤¾åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_company_like = False
            for pattern in company_patterns:
                if re.match(pattern, company, re.IGNORECASE):
                    is_company_like = True
                    break

            # ä¼šç¤¾åã‚‰ã—ããªã„å ´åˆã¯ã€é•·ã•ã¨å†…å®¹ã§åˆ¤æ–­
            if not is_company_like:
                # 3-20æ–‡å­—ã§ã€å¤§æ–‡å­—å°æ–‡å­—ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã¯ä¼šç¤¾åã®å¯èƒ½æ€§
                if (3 <= len(company) <= 20 and
                    any(c.isupper() for c in company) and
                    any(c.islower() for c in company)):
                    # è¿½åŠ ãƒã‚§ãƒƒã‚¯: æ˜ã‚‰ã‹ãªUIè¦ç´ ã§ãªã„ã“ã¨ã‚’ç¢ºèªï¼ˆå³ã—ã•ã‚’èª¿æ•´ï¼‰
                    obvious_ui_elements = ['website', 'location', 'chevron', 'right', 'left', 'up', 'down',
                                          'general', 'partner', 'lead', 'position', 'principal', 'associate',
                                          'founder', 'ceo', 'cto', 'cfo', 'coo', 'director', 'manager',
                                          'team', 'member', 'staff', 'employee', 'consultant', 'advisor',
                                          'board', 'committee', 'council', 'group', 'division', 'department',
                                          'section', 'unit', 'branch', 'office', 'location', 'address',
                                          'phone', 'email', 'contact', 'support', 'help', 'info', 'information',
                                          'service', 'services', 'product', 'products', 'solution', 'solutions',
                                          'technology', 'technologies', 'innovation', 'research', 'development',
                                          'investment', 'investments', 'portfolio', 'portfolios', 'company', 'companies',
                                          'corporation', 'corporations', 'limited', 'incorporated', 'partnership',
                                          'venture', 'ventures', 'capital', 'fund', 'funds', 'asset', 'assets',
                                          'management', 'consulting', 'advisory', 'financial', 'banking',
                                          'insurance', 'real estate', 'property', 'development', 'construction',
                                          'manufacturing', 'production', 'distribution', 'retail', 'wholesale',
                                          'trade', 'commerce', 'business', 'enterprise', 'startup', 'startups',
                                          'scaleup', 'scaleups', 'growth', 'expansion', 'acquisition', 'merger',
                                          'exit', 'exits', 'ipo', 'm&a', 'ma', 'deal', 'deals', 'transaction',
                                          'round', 'series', 'seed', 'angel', 'pre-seed', 'pre-seed', 'pre-seed']

                    if not any(ui_element in company_lower for ui_element in obvious_ui_elements):
                        is_company_like = True

                # è¿½åŠ : æ—¥æœ¬èªã®ä¼šç¤¾åãƒ‘ã‚¿ãƒ¼ãƒ³
                elif (3 <= len(company) <= 30 and
                      any(ord(c) > 127 for c in company)):  # éASCIIæ–‡å­—ï¼ˆæ—¥æœ¬èªãªã©ï¼‰ã‚’å«ã‚€
                    is_company_like = True

            if is_company_like:
                filtered_companies.add(company)

        return filtered_companies

    def scrape_with_requests(self, url: str) -> Optional[BeautifulSoup]:
        """
        requestsã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—

        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL

        Returns:
            BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return BeautifulSoup(response.content, 'lxml')
        except Exception as e:
            logger.error(f"requestsã§HTMLå–å¾—ã«å¤±æ•—: {url} - {e}")
            return None

    def scrape_with_selenium(self, url: str) -> Optional[BeautifulSoup]:
        """
        Seleniumã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—ï¼ˆJavaScriptãŒå¿…è¦ãªå ´åˆï¼‰

        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL

        Returns:
            BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        if not self.driver:
            logger.error("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None

        try:
            self.driver.get(url)
            WebDriverWait(self.driver, self.timeout).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(3)  # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
            return BeautifulSoup(self.driver.page_source, 'lxml')
        except Exception as e:
            logger.error(f"Seleniumã§HTMLå–å¾—ã«å¤±æ•—: {url} - {e}")
            return None

    def scrape_url(self, url: str) -> Dict[str, any]:
        """
        å˜ä¸€URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆï¼‰

        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL

        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®è¾æ›¸
        """
        logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")

        result = {
            'url': url,
            'portfolio_url': None,
            'companies': [],
            'error': None,
            'method': None,
            'ocr_used': False
        }

        # ã¾ãšrequestsã§è©¦è¡Œ
        soup = self.scrape_with_requests(url)
        if soup:
            result['method'] = 'requests'
        else:
            # requestsãŒå¤±æ•—ã—ãŸå ´åˆã€Seleniumã§è©¦è¡Œ
            soup = self.scrape_with_selenium(url)
            if soup:
                result['method'] = 'selenium'
            else:
                result['error'] = "HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                return result

        # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¿ãƒ–ã‚’æ¢ã™
        portfolio_url = self.find_portfolio_tab(soup, url)
        if portfolio_url:
            result['portfolio_url'] = portfolio_url

            # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            portfolio_soup = self.scrape_with_requests(portfolio_url)
            if not portfolio_soup:
                portfolio_soup = self.scrape_with_selenium(portfolio_url)

            if portfolio_soup:
                companies = self.extract_companies_from_page(portfolio_soup, portfolio_url)
                result['companies'] = list(companies)
                result['ocr_used'] = self.use_ocr
                logger.info(f"Portfolioãƒšãƒ¼ã‚¸ã‹ã‚‰ {len(companies)} ç¤¾ã®ä¼šç¤¾åã‚’æŠ½å‡º: {portfolio_url}")
            else:
                result['error'] = "Portfolioãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
        else:
            # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚¿ãƒ–ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ä¼šç¤¾åã‚’æŠ½å‡º
            companies = self.extract_companies_from_page(soup, url)
            result['companies'] = list(companies)
            result['ocr_used'] = self.use_ocr
            logger.info(f"ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ {len(companies)} ç¤¾ã®ä¼šç¤¾åã‚’æŠ½å‡º: {url}")

        return result

    def scrape_urls(self, urls: List[str]) -> List[Dict[str, any]]:
        """
        URLãƒªã‚¹ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            urls: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URLãƒªã‚¹ãƒˆ

        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []

        for i, url in enumerate(urls, 1):
            logger.info(f"é€²æ—: {i}/{len(urls)} - {url}")

            result = self.scrape_url(url)
            results.append(result)

            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å°‘ã—å¾…æ©Ÿ
            time.sleep(2)

        return results

    def save_results(self, results: List[Dict[str, any]], output_file: str = 'portfolio_results.json'):
        """
        çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            results: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ãƒªã‚¹ãƒˆ
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        except Exception as e:
            logger.error(f"çµæœã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def save_results_csv(self, results: List[Dict[str, any]], output_file: str = 'portfolio_results.csv'):
        """
        çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            results: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ãƒªã‚¹ãƒˆ
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        try:
            # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            flat_data = []
            for result in results:
                if result['companies']:
                    for company in result['companies']:
                        flat_data.append({
                            'url': result['url'],
                            'portfolio_url': result['portfolio_url'],
                            'company_name': company,
                            'method': result['method'],
                            'ocr_used': result['ocr_used'],
                            'error': result['error']
                        })
                else:
                    flat_data.append({
                        'url': result['url'],
                        'portfolio_url': result['portfolio_url'],
                        'company_name': '',
                        'method': result['method'],
                        'ocr_used': result['ocr_used'],
                        'error': result['error']
                    })

            df = pd.DataFrame(flat_data)
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"çµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        except Exception as e:
            logger.error(f"CSVä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.driver:
            self.driver.quit()
        self.session.close()
        logger.info("ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

def load_urls_from_file(filename: str) -> List[str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿

    Args:
        filename: URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        URLãƒªã‚¹ãƒˆ
    """
    urls = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'):
                    urls.append(url)
        logger.info(f"{len(urls)}å€‹ã®URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filename}")
    except Exception as e:
        logger.error(f"URLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    return urls

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ã‚µãƒ³ãƒ—ãƒ«URLãƒªã‚¹ãƒˆï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    sample_urls = [
        "https://example.com",
        "https://example.org",
        # å®Ÿéš›ã®URLã‚’ã“ã“ã«è¿½åŠ 
    ]

    # URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯èª­ã¿è¾¼ã¿
    url_file = 'urls.txt'
    try:
        urls = load_urls_from_file(url_file)
        if not urls:
            logger.warning(f"URLãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã¾ãŸã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url_file}")
            logger.info("ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨ã—ã¾ã™")
            urls = sample_urls
    except FileNotFoundError:
        logger.info(f"URLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url_file}")
        logger.info("ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨ã—ã¾ã™")
        urls = sample_urls

    if not urls:
        logger.error("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URLãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–ï¼ˆOCRæ©Ÿèƒ½ä»˜ãï¼‰
    scraper = PortfolioScraper(headless=True, timeout=20, use_ocr=True)

    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        results = scraper.scrape_urls(urls)

        # çµæœã®ä¿å­˜
        scraper.save_results(results, 'portfolio_results.json')
        scraper.save_results_csv(results, 'portfolio_results.csv')

        # çµæœã®è¡¨ç¤º
        total_companies = sum(len(result['companies']) for result in results)
        successful_scrapes = sum(1 for result in results if not result['error'])
        ocr_used_count = sum(1 for result in results if result['ocr_used'])

        logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†:")
        logger.info(f"  å‡¦ç†ã—ãŸURLæ•°: {len(urls)}")
        logger.info(f"  æˆåŠŸã—ãŸURLæ•°: {successful_scrapes}")
        logger.info(f"  æŠ½å‡ºã•ã‚ŒãŸä¼šç¤¾åæ•°: {total_companies}")
        logger.info(f"  OCRä½¿ç”¨å›æ•°: {ocr_used_count}")

    except KeyboardInterrupt:
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        scraper.close()

if __name__ == "__main__":
    main()
